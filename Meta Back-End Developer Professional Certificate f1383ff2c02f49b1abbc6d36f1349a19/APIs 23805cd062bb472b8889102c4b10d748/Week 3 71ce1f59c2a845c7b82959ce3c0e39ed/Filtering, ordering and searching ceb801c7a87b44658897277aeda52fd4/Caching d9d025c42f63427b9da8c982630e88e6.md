# Caching

- Caching is a technique of serving saved results instead of creating a fresh one every time it is requested
- Improves server load and bandwidth consumption
- Can be done in multiple layers of the API infrastructure

# Caching in Different Layers

## Database Server

- Most modern databases do caching to prevent excessive read-write operation in the actual storage
- SQL queries and their results are stored in memory
- Relies only on database caching is not enough because server-side scripts still connect to the database

## Web Server

- Server-side scripts can cache response if there is no change in data
- Result is cached in a separate storage (files, database, Redis, Memcached)
- Reduces database connection, saves processing time and computing power

## Reverse Proxy Server

- Distributes requests evenly in traffic heavy applications
- Web servers send responses with appropriate caching headers
- Reverse proxy caches result for a certain amount of time as mentioned in headers
- Serves requests straight from cache, reducing burden on web servers

## Client-side Caching

- Reverse proxies or web servers send responses with caching headers to client
- Client browser or application decides whether to cache or create a call to the server
- Implementing proper caching strategy on the server side is important

# Conclusion

In this video, you learned about the different ways to implement caching in your API infrastructure to improve server performance.